summary(this.mod$model)
T500_vars <- read.csv("Data/T500_redcap_wide-S.T0-2019-02-18.csv", row.names = 1, stringsAsFactors = F)
T500_vars <- T500_vars[colnames(T500_PRS)[-c(1:5)],]
T500_PRS <- read.csv("T500_PRS.csv", stringsAsFactors = F)
if(!dir.exists("Reports")){dir.create("Reports")}
dom <- unique(unlist(lapply(sapply(T500_PRS$file, strsplit, "/"), "[", 1)))
dom_folder_paths <- paste0("Reports/", dom)
dom_folder_paths <- dom_folder_paths[!dir.exists(dom_folder_paths)]
sapply(dom_folder_paths, dir.create)
T500_vars <- read.csv("Data/T500_redcap_wide-S.T0-2019-02-18.csv", row.names = 1, stringsAsFactors = F)
T500_vars <- T500_vars[colnames(T500_PRS)[-c(1:5)],]
col.gen <- ifelse(T500_vars$Gender == "Male", "blue", "red")
n <- length(T500_PRS)
View(T500_PRS)
T500_PRS$file
header(T500_vars,1)
T500_vars[1,]
colnames(T500_vars)
# Reading in structural data. This file is very large, so it is immediately subsetted to the variables
# that will be used later to save memory.
idps.data <- read.csv("Data/idps-SFRE-subset-12-16-2019.csv", stringsAsFactors = F)
# Adding an extra variable that estimates cerebrospinal fluid.
idps.data$est_CSF <- idps.data$srage_freesurfer_summary_BrainSegVol - idps.data$srage_freesurfer_summary_BrainSegVolNotVent
# Merging the structural data with the main T1000 dataset
T500_vars <- merge(T500_vars, idps.data, by = "id")
T500_vars <- read.csv("Data/T500_redcap_wide-S.T0-2019-02-18.csv", row.names = 1, stringsAsFactors = F)
# Reading in structural data. This file is very large, so it is immediately subsetted to the variables
# that will be used later to save memory.
idps.data <- read.csv("Data/idps-SFRE-subset-12-16-2019.csv", stringsAsFactors = F)
# Adding an extra variable that estimates cerebrospinal fluid.
idps.data$est_CSF <- idps.data$srage_freesurfer_summary_BrainSegVol - idps.data$srage_freesurfer_summary_BrainSegVolNotVent
# Merging the structural data with the main T1000 dataset
T500_vars <- merge(T500_vars, idps.data, by = "id")
colnames(T500_vars)
# Merging the structural data with the main T1000 dataset
T500_vars <- merge(T500_vars, idps.data, by.y = "id", by.x = 0)
T500_vars
T500_vars <- read.csv("Data/T500_redcap_wide-S.T0-2019-02-18.csv", row.names = 1, stringsAsFactors = F)
T500_vars <- T500_vars[colnames(T500_PRS)[-c(1:5)],]
# Reading in structural data. This file is very large, so it is immediately subsetted to the variables
# that will be used later to save memory.
idps.data <- read.csv("Data/idps-SFRE-subset-12-16-2019.csv", stringsAsFactors = F)
# Adding an extra variable that estimates cerebrospinal fluid.
idps.data$est_CSF <- idps.data$srage_freesurfer_summary_BrainSegVol - idps.data$srage_freesurfer_summary_BrainSegVolNotVent
# Merging the structural data with the main T1000 dataset
T500_vars <- merge(T500_vars, idps.data, by.y = "id", by.x = 0)
T500_PRS <- read.csv("T500_PRS.csv", stringsAsFactors = F)
if(!dir.exists("Reports")){dir.create("Reports")}
dom <- unique(unlist(lapply(sapply(T500_PRS$file, strsplit, "/"), "[", 1)))
dom_folder_paths <- paste0("Reports/", dom)
dom_folder_paths <- dom_folder_paths[!dir.exists(dom_folder_paths)]
sapply(dom_folder_paths, dir.create)
T500_vars <- read.csv("Data/T500_redcap_wide-S.T0-2019-02-18.csv", row.names = 1, stringsAsFactors = F)
T500_vars <- T500_vars[colnames(T500_PRS)[-c(1:5)],]
# Reading in structural data. This file is very large, so it is immediately subsetted to the variables
# that will be used later to save memory.
idps.data <- read.csv("Data/idps-SFRE-subset-12-16-2019.csv", stringsAsFactors = F)
# Adding an extra variable that estimates cerebrospinal fluid.
idps.data$est_CSF <- idps.data$srage_freesurfer_summary_BrainSegVol - idps.data$srage_freesurfer_summary_BrainSegVolNotVent
# Merging the structural data with the main T1000 dataset
T500_vars <- merge(T500_vars, idps.data, by.y = "id", by.x = 0)
col.gen <- ifelse(T500_vars$Gender == "Male", "blue", "red")
n <- length(T500_PRS)
T500_PRS$file
n <- dim(T500_PRS)[1]
T500_PRS_score()
T500_PRS <- read.csv("T500_PRS.csv", stringsAsFactors = F)
if(!dir.exists("Reports")){dir.create("Reports")}
setwd("/Volumes")
setwd("/Volumes/T1000/Analysis/kforthman/optmThrGFA")
knitr::opts_chunk$set(echo = TRUE)
library(GFA)
library(knitr)
library(gplots)
library(ggplot2)
library(foreach)
opts <- getDefaultOpts()
opts$convergenceCheck <- T
library(devtools)
install_github("kforthman/optmThrGFA")
library(optmThrGFA)
block.length <- c(15,10,5,10)
n.blocks <- length(block.length)
varIdx.by.block <- lapply(
1:n.blocks,
function(x){c(1:block.length[x]) + sum(block.length[1:x-1])}
)
block.names <- paste0("Block_", 1:n.blocks)
block4vars <-unlist(lapply(1:n.blocks, function(x){rep(block.names[x], block.length[x])}))
corGrids <- matchGrids <- c(seq(0.1, 0.5, by=0.2), seq(0.6, 0.9, 0.1))
plot(tanh(rnorm(7*40)))
paste0("K", 1:ncol(W))
W <- matrix(tanh(rnorm(7*40)), 40, 7)
paste0("K", 1:ncol(W))
paste0("V", 1:nrow(W))
W <- matrix(tanh(rnorm(K*D)), D, K)
K <- 7
D <- 40
W <- matrix(tanh(rnorm(K*D)), D, K)
colnames(W) <- paste0("K", 1:K)
rownames(W) <- paste0("V", 1:D
)
varIdx.by.block
# Create an example posterior matrix with random data.
#
W <- matrix(tanh(rnorm(K*D)), D, K)
colnames(W) <- paste0("K", 1:K)
colnames(W) <- paste0("K", 1:K)
rownames(W) <- paste0("V", 1:D)
W_heatmap(W_DxK=W, varIdx.by.block, block.names)
set.seed(123)
K <- 7
D <- 40
# Create an example posterior matrix with random data.
#
W <- matrix(tanh(rnorm(K*D)), D, K)
colnames(W) <- paste0("K", 1:K)
rownames(W) <- paste0("V", 1:D)
W_heatmap(W_DxK=W, varIdx.by.block, block.names)
W[c(varIdx.by.block[[2]], varIdx.by.block[[3]]), 2] <- 0
W[c(varIdx.by.block[[1]], varIdx.by.block[[4]]), 3] <- 0
W[c(varIdx.by.block[[2]], varIdx.by.block[[3]], varIdx.by.block[[4]]), 4] <- 0
W[c(varIdx.by.block[[1]], varIdx.by.block[[3]], varIdx.by.block[[4]]), 5] <- 0
W[c(varIdx.by.block[[1]], varIdx.by.block[[2]], varIdx.by.block[[4]]), 6] <- 0
W[c(varIdx.by.block[[1]], varIdx.by.block[[2]], varIdx.by.block[[3]]), 7] <- 0
# round(tmp, 1)
W_heatmap(W_DxK=W, varIdx.by.block, block.names)
D <- sum(block.length)
D
varIdx.by.block
set.seed(123)
K <- 7
D <- sum(block.length)
# Create an example posterior matrix with random data.
#
W <- matrix(tanh(rnorm(K*D)), D, K)
colnames(W) <- paste0("K", 1:K)
rownames(W) <- paste0("V", 1:D)
W_heatmap(W_DxK=W, varIdx.by.block, block.names)
W[c(varIdx.by.block[[2]], varIdx.by.block[[3]]), 2] <- 0
W[c(varIdx.by.block[[1]], varIdx.by.block[[4]]), 3] <- 0
W[c(varIdx.by.block[[2]], varIdx.by.block[[3]], varIdx.by.block[[4]]), 4] <- 0
W[c(varIdx.by.block[[1]], varIdx.by.block[[3]], varIdx.by.block[[4]]), 5] <- 0
W[c(varIdx.by.block[[1]], varIdx.by.block[[2]], varIdx.by.block[[4]]), 6] <- 0
W[c(varIdx.by.block[[1]], varIdx.by.block[[2]], varIdx.by.block[[3]]), 7] <- 0
# round(tmp, 1)
W_heatmap(W_DxK=W, varIdx.by.block, block.names)
K.blocks <- c(
1,1,1,1,
1,0,0,1,
0,1,1,0,
1,0,0,0,
0,1,0,0,
0,0,1,0,
0,0,0,1
)
W[varIdx.by.block[[K.blocks[,1]]]],1]
W[varIdx.by.block[[K.blocks[,1]]],1]
K.blocks <- matrix(c(
1,1,1,1,
1,0,0,1,
0,1,1,0,
1,0,0,0,
0,1,0,0,
0,0,1,0,
0,0,0,1
), K, n.blocks)
K.blocks[1,]
K.blocks <- matrix(c(
1,1,1,1,
1,0,0,1,
0,1,1,0,
1,0,0,0,
0,1,0,0,
0,0,1,0,
0,0,0,1
), byrow = T, K, n.blocks)
K.blocks[1,]
varIdx.by.block[[K.blocks[1,]]]
library(optmThrGFA)
library(optmThrGFA)
library(optmThrGFA)
library(optmThrGFA)
knitr::opts_chunk$set(echo = TRUE)
veTrue <- rep(NA)
for(k in 1:K){
veTrue[k] <- mean(diag(as.matrix(W[, k]) %*% as.matrix(t(W[, k])))) * diag(var(dat$X))[k]
}
knitr::opts_chunk$set(echo = TRUE)
library(GFA)
library(knitr)
library(gplots)
library(ggplot2)
library(foreach)
opts <- getDefaultOpts()
opts$convergenceCheck <- T
library(devtools)
install_github("kforthman/optmThrGFA")
library(optmThrGFA)
# Define the number of variables in each block.
block.length <- c(15,10,5,10)
n.blocks <- length(block.length)
varIdx.by.block <- lapply(
1:n.blocks,
function(x){c(1:block.length[x]) + sum(block.length[1:x-1])}
)
block.names <- paste0("Block_", 1:n.blocks)
block4vars <-unlist(lapply(1:n.blocks, function(x){rep(block.names[x], block.length[x])}))
corGrids <- matchGrids <- c(seq(0.1, 0.5, by=0.2), seq(0.6, 0.9, 0.1))
set.seed(123)
K <- 7
D <- sum(block.length)
# Create an example posterior matrix with random data.
W <- matrix(tanh(rnorm(K*D)), D, K)
colnames(W) <- paste0("K", 1:K)
rownames(W) <- paste0("V", 1:D)
# K.blocks defines what blocks load into what factors.
# Columns are blocks, factors are rows.
# 1 indicates the block loads on the factor.
K.blocks <- matrix(c(
# Factor 1
1,1,1,1, # blocks 1,2,3,4
# Factor 2
1,0,0,1, # blocks 1,4
# Factor 3
0,1,1,0, # blocks 2,3
# Factor 4
1,0,0,0, # blocks 1
# Factor 5
0,1,0,0, # blocks 2
# Factor 6
0,0,1,0, # blocks 3
# Factor 7
0,0,0,1  # blocks 4
) == 1, byrow = T, K, n.blocks)
# Set designated non-loading blocks to 0.
for(i in 1:K){
W[unlist(varIdx.by.block[!K.blocks[i,]]),i] <- 0
}
# View the results
W_heatmap(W_DxK=W, varIdx.by.block, block.names)
# View the results
W_heatmap(W_DxK=W, varIdx.by.block, block.names)
knitr::opts_chunk$set(echo = TRUE)
library(GFA)
library(knitr)
library(gplots)
library(ggplot2)
library(foreach)
opts <- getDefaultOpts()
opts$convergenceCheck <- T
library(optmThrGFA)
knitr::opts_chunk$set(echo = TRUE)
# Define the number of variables in each block.
block.length <- c(15,10,5,10)
n.blocks <- length(block.length)
varIdx.by.block <- lapply(
1:n.blocks,
function(x){c(1:block.length[x]) + sum(block.length[1:x-1])}
)
block.names <- paste0("Block_", 1:n.blocks)
block4vars <-unlist(lapply(1:n.blocks, function(x){rep(block.names[x], block.length[x])}))
corGrids <- matchGrids <- c(seq(0.1, 0.5, by=0.2), seq(0.6, 0.9, 0.1))
set.seed(123)
K <- 7
D <- sum(block.length)
# Create an example posterior matrix with random data.
W <- matrix(tanh(rnorm(K*D)), D, K)
colnames(W) <- paste0("K", 1:K)
rownames(W) <- paste0("V", 1:D)
# K.blocks defines what blocks load into what factors.
# Columns are blocks, factors are rows.
# 1 indicates the block loads on the factor.
K.blocks <- matrix(c(
# Factor 1
1,1,1,1, # blocks 1,2,3,4
# Factor 2
1,0,0,1, # blocks 1,4
# Factor 3
0,1,1,0, # blocks 2,3
# Factor 4
1,0,0,0, # blocks 1
# Factor 5
0,1,0,0, # blocks 2
# Factor 6
0,0,1,0, # blocks 3
# Factor 7
0,0,0,1  # blocks 4
) == 1, byrow = T, K, n.blocks)
# Set designated non-loading blocks to 0.
for(i in 1:K){
W[unlist(varIdx.by.block[!K.blocks[i,]]),i] <- 0
}
# View the results
W_heatmap(W_DxK=W, varIdx.by.block, block.names)
if(!file.exists("res")){dir.create("res")}
folder <- "N100_D40_K7_noise0"
if(!file.exists(paste0("res/",folder))){dir.create(paste0("res/",folder))}
N <- 100
set.seed(123)
# Simulate the variable values for each observation assuming the
# underlying structure defined by W.
dat <- data.simu(N = N, W_DxK = W, varIdx.by.block, sd.noise = 0)
names(dat$Y.list) <- block.names
mynorm <- normalizeData(train=dat$Y.list, type="scaleFeatures")
Y <- do.call(cbind, mynorm$train)
veTrue <- rep(NA)
for(k in 1:K){
veTrue[k] <- mean(diag(as.matrix(W[, k]) %*% as.matrix(t(W[, k])))) * diag(var(dat$X))[k]
}
veTrue <- matrix(nrow = K, ncol = 1)
W[, k]
W[, k] %*% t(W[, k])
W[, k] %*% t(W[, k]))
mean(
diag(
W[, k] %*% t(W[, k])
)
)
knitr::opts_chunk$set(echo = TRUE)
library(GFA)
library(knitr)
library(gplots)
library(ggplot2)
library(foreach)
opts <- getDefaultOpts()
opts$convergenceCheck <- T
library(devtools)
#install_github("kforthman/optmThrGFA")
library(optmThrGFA)
# Define the number of variables in each block.
block.length <- c(15,10,5,10)
n.blocks <- length(block.length)
varIdx.by.block <- lapply(
1:n.blocks,
function(x){c(1:block.length[x]) + sum(block.length[1:x-1])}
)
block.names <- paste0("Block_", 1:n.blocks)
block4vars <-unlist(lapply(1:n.blocks, function(x){rep(block.names[x], block.length[x])}))
corGrids <- matchGrids <- c(seq(0.1, 0.5, by=0.2), seq(0.6, 0.9, 0.1))
set.seed(123)
K <- 7
D <- sum(block.length)
# Create an example posterior matrix with random data.
W <- matrix(tanh(rnorm(K*D)), D, K)
colnames(W) <- paste0("K", 1:K)
rownames(W) <- paste0("V", 1:D)
# K.blocks defines what blocks load into what factors.
# Columns are blocks, factors are rows.
# 1 indicates the block loads on the factor.
K.blocks <- matrix(c(
# Factor 1
1,1,1,1, # blocks 1,2,3,4
# Factor 2
1,0,0,1, # blocks 1,4
# Factor 3
0,1,1,0, # blocks 2,3
# Factor 4
1,0,0,0, # blocks 1
# Factor 5
0,1,0,0, # blocks 2
# Factor 6
0,0,1,0, # blocks 3
# Factor 7
0,0,0,1  # blocks 4
) == 1, byrow = T, K, n.blocks)
# Set designated non-loading blocks to 0.
for(i in 1:K){
W[unlist(varIdx.by.block[!K.blocks[i,]]),i] <- 0
}
# View the results
W_heatmap(W_DxK=W, varIdx.by.block, block.names)
if(!file.exists("res")){dir.create("res")}
folder <- "N100_D40_K7_noise0"
if(!file.exists(paste0("res/",folder))){dir.create(paste0("res/",folder))}
N <- 100
set.seed(123)
# Simulate the variable values for each observation assuming the
# underlying structure defined by W.
dat <- data.simu(N = N, W_DxK = W, varIdx.by.block, sd.noise = 0)
names(dat$Y.list) <- block.names
mynorm <- normalizeData(train=dat$Y.list, type="scaleFeatures")
Y <- do.call(cbind, mynorm$train)
mean(
diag(
W[, k] %*% t(W[, k])
)
)
k <- 1
mean(
diag(
W[, k] %*% t(W[, k])
)
)
W[, k] %*% t(W[, k])
test <- c("a", "b", "c")
test %*% t(test)
test <- as.matrix(c("a", "b", "c"))
test %*% t(test)
test <- as.matrix(c(1,2,3))
test %*% t(test)
W[,k]^2
W[,k]
diag(var(dat$X[,k])) == diag(var(dat$X))[k]
dat$X[,k]
var(dat$X[,k])
var(dat$X
)
var(dat$X[,k]) == diag(var(dat$X))[k]
veTrue <- matrix(nrow = K, ncol = 1)
veTrue <- matrix(nrow = K, ncol = 1)
for(k in 1:K){
veTrue[k] <-
mean(
diag(
W[, k] %*% t(W[, k])
)
) * diag(var(dat$X))[k]
}
for(k in 1:K){
veTrue[k] <-
mean(
diag(
W[, k] %*% t(W[, k])
)
) * var(dat$X[,k])
}
round(veTrue, 2)
round(veTrue, 2)
sum(veTrue)
W[, k] %*% t(W[, k]) == var(W[,k])
(W[, k] %*% t(W[, k])) == cov(W[,k])
(W[, k] %*% t(W[, k])) == cor(W[,k])
(W[, k] %*% t(W[, k])) == var(W[,k])
var(W[,k])
W[,k]
W[, k]
veTrue <- matrix(nrow = K, ncol = 1)
for(k in 1:K){
veTrue[k] <-
mean(
diag(
W[, k] %*% t(W[, k])
)
) * var(dat$X[,k])
}
round(veTrue, 2)
sum(veTrue)
apply(W^2, 2, mean)
sum(apply(W^2, 2, mean))
round(apply(dat$X, 2, sd), 2)
diag(
W[, k] %*% t(W[, k])
)
W[, k]
round(veTrue, 2)
sum(veTrue)
apply(W^2, 2, mean)
var(dat$X[,k])
mean(
diag(
W[, k] %*% t(W[, k])
)
)
mean(
diag(
W[, k] %*% t(W[, k])
)
) * var(dat$X[,k])
e <- eigen(W)
# R is the number of replicates
R <- 10
foreach(r = 1:R, .packages=c("GFA")) %dopar% {
this.filename <- paste0("res/", folder, "/GFA_rep_", r, ".rda")
if(!file.exists(this.filename)){
set.seed(r)
res <- gfa(mynorm$train, opts=opts, K=10)
save(res, file = this.filename)
}
}
# R is the number of replicates
R <- 10
foreach(r = 1:R, .packages=c("GFA")) %dopar% {
this.filename <- paste0("res/", folder, "/GFA_rep_", r, ".rda")
if(!file.exists(this.filename)){
set.seed(r)
res <- gfa(Y, opts=opts, K=10)
save(res, file = this.filename)
}
}
# R is the number of replicates
R <- 10
for(r in 1:R){
this.filename <- paste0("res/", folder, "/GFA_rep_", r, ".rda")
if(!file.exists(this.filename)){
set.seed(r)
res <- gfa(Y, opts=opts, K=10)
save(res, file = this.filename)
}
}
# R is the number of replicates
R <- 10
for(r in 1:R){
this.filename <- paste0("res/", folder, "/GFA_rep_", r, ".rda")
if(!file.exists(this.filename)){
set.seed(r)
res <- gfa(Y, opts=opts, K=10)
save(res, file = this.filename)
}
}
res
res <- gfa(Y, opts=opts, K=10)
